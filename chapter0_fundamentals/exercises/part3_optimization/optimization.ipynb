{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# [0.3] Optimization & Hyperparameters"
      ],
      "metadata": {
        "id": "c5quWvWTL-Z7"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ROHJak312DL"
      },
      "source": [
        "Colab: [exercises](https://colab.research.google.com/drive/1iCM3V0G3B7NSxKsZkvHtmv7k9rgBXYk4) | [solutions](https://colab.research.google.com/drive/1HABl0_vi0AokGVk8-wv7KX6Csd65ZSmr)\n",
        "\n",
        "ARENA 3.0 [Streamlit page](https://arena3-chapter0-fundamentals.streamlit.app/[0.3]_Optimization)\n",
        "\n",
        "Please send any problems / bugs on the `#errata` channel in the [Slack group](https://join.slack.com/t/arena-la82367/shared_invite/zt-1uvoagohe-JUv9xB7Vr143pdx1UBPrzQ), and ask any questions on the dedicated channels for this chapter of material."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1eUoAXhRa3-"
      },
      "source": [
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/stats.png\" width=\"350\">\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0502-wyRh8A"
      },
      "source": [
        "# Introduction\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t8tJizi3BOcP"
      },
      "source": [
        "In today's exercises, we will explore various optimization algorithms and their roles in training deep learning models. We will delve into the inner workings of different optimization techniques such as Stochastic Gradient Descent (SGD), RMSprop, and Adam, and learn how to implement them using code. Additionally, we will discuss the concept of loss landscapes and their significance in visualizing the challenges faced during the optimization process. By the end of this set of exercises, you will have a solid understanding of optimization algorithms and their impact on model performance. We'll also take a look at Weights and Biases, a tool that can be used to track and visualize the training process, and test different values of hyperparameters to find the most effective ones.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcewkMZkBOcP"
      },
      "source": [
        "## Content & Learning Objectives\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErviqJ4vRm15"
      },
      "source": [
        "### 1Ô∏è‚É£ Optimizers\n",
        "\n",
        "These exercises will take you through how different optimization algorithms work (specifically SGD, RMSprop and Adam). You'll write your own optimisers, and use plotting functions to visualise gradient descent on loss landscapes.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand how different optimization algorithms work\n",
        "> * Translate pseudocode for these algorithms into code\n",
        "> * Understand the idea of loss landscapes, and how they can visualize specific challenges in the optimization process\n",
        "\n",
        "### 2Ô∏è‚É£ Weights and Biases\n",
        "\n",
        "In this section, we'll look at methods for choosing hyperparameters effectively. You'll learn how to use **Weights and Biases**, a useful tool for hyperparameter search. By the end of today, you should be able to use Weights and Biases to train the ResNet you created in the last set of exercises.\n",
        "\n",
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Learn what the most important hyperparameters are, and methods for efficiently searching over hyperparameter space\n",
        "> * Adapt your code from yesterday to log training runs to Weights & Biases, and use this service to run hyperparameter sweeps\n",
        "\n",
        "### 3Ô∏è‚É£ Bonus\n",
        "\n",
        "This section gives you suggestions for further exploration of optimizers, and Weights & Biases.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Na3PD6GwlBSk"
      },
      "source": [
        "## Setup (don't read, just run!)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Za2YIkzBOcQ"
      },
      "outputs": [],
      "source": [
        "try:\n",
        "    import google.colab\n",
        "    IN_COLAB = True\n",
        "except:\n",
        "    IN_COLAB = False\n",
        "\n",
        "if IN_COLAB:\n",
        "    # Install packages\n",
        "    %pip install einops\n",
        "    %pip install jaxtyping\n",
        "    %pip install torchinfo\n",
        "    %pip install wandb\n",
        "\n",
        "    # Code to make sure output widgets display\n",
        "    from google.colab import output\n",
        "    output.enable_custom_widget_manager()\n",
        "\n",
        "    # Code to download the necessary files (e.g. solutions, test funcs)\n",
        "    import os, sys\n",
        "    if not os.path.exists(\"chapter0_fundamentals\"):\n",
        "        !wget https://github.com/callummcdougall/ARENA_3.0/archive/refs/heads/main.zip\n",
        "        !unzip /content/main.zip 'ARENA_3.0-main/chapter0_fundamentals/exercises/*'\n",
        "        os.remove(\"/content/main.zip\")\n",
        "        os.rename(\"ARENA_3.0-main/chapter0_fundamentals\", \"chapter0_fundamentals\")\n",
        "        os.rmdir(\"ARENA_3.0-main\")\n",
        "        sys.path.insert(0, \"chapter0_fundamentals/exercises\")\n",
        "\n",
        "    # Clear output\n",
        "    from IPython.display import clear_output\n",
        "    clear_output()\n",
        "    print(\"Imports & installations complete!\")\n",
        "\n",
        "else:\n",
        "    from IPython import get_ipython\n",
        "    ipython = get_ipython()\n",
        "    ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
        "    ipython.run_line_magic(\"autoreload\", \"2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9IS6vS4lEo4"
      },
      "outputs": [],
      "source": [
        "import os; os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
        "import sys\n",
        "import torch as t\n",
        "from torch import Tensor, optim\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from typing import Callable, Iterable, Tuple, Optional\n",
        "from jaxtyping import Float\n",
        "from dataclasses import dataclass, replace\n",
        "from tqdm.notebook import tqdm\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Get file paths to this set of exercises\n",
        "exercises_dir = Path(\"chapter0_fundamentals/exercises\")\n",
        "section_dir = exercises_dir / \"part3_optimization\"\n",
        "\n",
        "from plotly_utils import bar, imshow, plot_train_loss_and_test_accuracy_from_trainer\n",
        "from part2_cnns.solutions import IMAGENET_TRANSFORM, ResNet34\n",
        "from part2_cnns.solutions_bonus import get_resnet_for_feature_extraction\n",
        "from part3_optimization.utils import plot_fn, plot_fn_with_points\n",
        "import part3_optimization.tests as tests\n",
        "\n",
        "device = t.device(\"cuda\" if t.cuda.is_available() else \"cpu\")\n",
        "\n",
        "MAIN = __name__ == \"__main__\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pSCYV7G4BOcR"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I get a NumPy-related error</summary>\n",
        "\n",
        "This is an annoying colab-related issue which I haven't been able to find a satisfying fix for. If you restart runtime (but don't delete runtime), and run just the imports cell above again (but not the `%pip install` cell), the problem should go away.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqyzRF6yh8-"
      },
      "source": [
        "# 1Ô∏è‚É£ Optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLZ6nMkjBOcS"
      },
      "source": [
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Understand how different optimization algorithms work\n",
        "> * Translate pseudocode for these algorithms into code\n",
        "> * Understand the idea of loss landscapes, and how they can visualize specific challenges in the optimization process\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vc7ssroMy4Pl"
      },
      "source": [
        "## Reading\n",
        "\n",
        "Some of these are strongly recommended, while others are optional. If you like, you can jump back to some of these videos while you're going through the material, if you feel like you need to.\n",
        "\n",
        "* Andrew Ng's video series on gradient descent variants: [Gradient Descent With Momentum](https://www.youtube.com/watch?v=k8fTYJPd3_I) (9 mins), [RMSProp](https://www.youtube.com/watch?v=_e-LFe_igno) (7 mins), [Adam](https://www.youtube.com/watch?v=JXQT_vxqwIs&list=PLkDaE6sCZn6Hn0vK8co82zjQtt3T2Nkqc&index=23) (7 mins)\n",
        "    * These videos are strongly recommended, especially the RMSProp video\n",
        "* [A Visual Explanation of Gradient Descent Methods](https://towardsdatascience.com/a-visual-explanation-of-gradient-descent-methods-momentum-adagrad-rmsprop-adam-f898b102325c)\n",
        "    * This is also strongly recommended; if you only want to read/watch one thing, make it this\n",
        "* [Why Momentum Really Works (distill.pub)](https://distill.pub/2017/momentum/)\n",
        "    * This is optional, but a fascinating read if you have time and are interested in engaging with the mathematical details of optimization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4Dk_cIHy6gv"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "Tomorrow, we'll look in detail about how the backpropagation algorithm works. But for now, let's take it as read that calling `loss.backward()` on a scalar `loss` will result in the computation of the gradients $\\frac{\\partial loss}{\\partial w}$ for every parameter `w` in the model, and store these values in `w.grad`. How do we use these gradients to update our parameters in a way which decreases loss?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YuGKE8oy-Cx"
      },
      "source": [
        "A loss function can be any differentiable function such that we prefer a lower value. To apply gradient descent, we start by initializing the parameters to random values (the details of this are subtle), and then repeatedly compute the gradient of the loss with respect to the model parameters. It [can be proven](https://tutorial.math.lamar.edu/Classes/CalcIII/DirectionalDeriv.aspx) that for an infinitesimal step, moving in the direction of the gradient would increase the loss by the largest amount out of all possible directions.\n",
        "\n",
        "We actually want to decrease the loss, so we subtract the gradient to go in the opposite direction. Taking infinitesimal steps is no good, so we pick some learning rate $\\lambda$ (also called the step size) and scale our step by that amount to obtain the update rule for gradient descent:\n",
        "\n",
        "$$\\theta_t \\leftarrow \\theta_{t-1} - \\lambda \\nabla L(\\theta_{t-1})$$\n",
        "\n",
        "We know that an infinitesimal step will decrease the loss, but a finite step will only do so if the loss function is linear enough in the neighbourhood of the current parameters. If the loss function is too curved, we might actually increase our loss.\n",
        "\n",
        "The biggest advantage of this algorithm is that for N bytes of parameters, you only need N additional bytes of memory to store the gradients, which are of the same shape as the parameters. GPU memory is very limited, so this is an extremely relevant consideration. The amount of computation needed is also minimal: one multiply and one add per parameter.\n",
        "\n",
        "The biggest disadvantage is that we're completely ignoring the curvature of the loss function, not captured by the gradient consisting of partial derivatives. Intuitively, we can take a larger step if the loss function is flat in some direction or a smaller step if it is very curved. Generally, you could represent this by some matrix P that pre-multiplies the gradients to rescale them to account for the curvature. $P$ is called a preconditioner, and gradient descent is equivalent to approximating $P$ by an identity matrix, which is a very bad approximation.\n",
        "\n",
        "Most competing optimizers can be interpreted as trying to do something more sensible for $P$, subject to the constraint that GPU memory is at a premium. In particular, constructing $P$ explicitly is infeasible, since it's an $N \\times N$ matrix and N can be hundreds of billions. One idea is to use a diagonal $P$, which only requires N additional memory. An example of a more sophisticated scheme is [Shampoo](https://arxiv.org/pdf/1802.09568.pdf).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcU8Jkiey_iL"
      },
      "source": [
        "> The algorithm is called **Shampoo** because you put shampoo on your hair before using conditioner, and this method is a pre-conditioner.\n",
        ">     \n",
        "> If you take away just one thing from this entire curriculum, please don't let it be this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BQuBaQTOzBu1"
      },
      "source": [
        "## Stochastic Gradient Descent\n",
        "\n",
        "The terms gradient descent and SGD are used loosely in deep learning. To be technical, there are three variations:\n",
        "\n",
        "- Batch gradient descent - the loss function is the loss over the entire dataset. This requires too much computation unless the dataset is small, so it is rarely used in deep learning.\n",
        "- Stochastic gradient descent - the loss function is the loss on a randomly selected example. Any particular loss may be completely in the wrong direction of the loss on the entire dataset, but in expectation it's in the right direction. This has some nice properties but doesn't parallelize well, so it is rarely used in deep learning.\n",
        "- Mini-batch gradient descent - the loss function is the loss on a batch of examples of size `batch_size`. This is the standard in deep learning.\n",
        "\n",
        "The class `torch.optim.SGD` can be used for any of these by varying the number of examples passed in. We will be using only mini-batch gradient descent in this course.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lStatNS0zC81"
      },
      "source": [
        "## Batch Size\n",
        "\n",
        "In addition to choosing a learning rate or learning rate schedule, we need to choose the batch size or batch size schedule as well. Intuitively, using a larger batch means that the estimate of the gradient is closer to that of the true gradient over the entire dataset, but this requires more compute. Each element of the batch can be computed in parallel so with sufficient compute, one can increase the batch size without increasing wall-clock time. For small-scale experiments, a good heuristic is thus \"fill up all of your GPU memory\".\n",
        "\n",
        "At a larger scale, we would expect diminishing returns of increasing the batch size, but empirically it's worse than that - a batch size that is too large generalizes more poorly in many scenarios. The intuition that a closer approximation to the true gradient is always better is therefore incorrect. See [this paper](https://arxiv.org/pdf/1706.02677.pdf) for one discussion of this.\n",
        "\n",
        "For a batch size schedule, most commonly you'll see batch sizes increase over the course of training. The intuition is that a rough estimate of the proper direction is good enough early in training, but later in training it's important to preserve our progress and not \"bounce around\" too much.\n",
        "\n",
        "You will commonly see batch sizes that are a multiple of 32. One motivation for this is that when using CUDA, threads are grouped into \"warps\" of 32 threads which execute the same instructions in parallel. So a batch size of 64 would allow two warps to be fully utilized, whereas a size of 65 would require waiting for a third warp to finish. As batch sizes become larger, this wastage becomes less important.\n",
        "\n",
        "Powers of two are also common - the idea here is that work can be recursively divided up among different GPUs or within a GPU. For example, a matrix multiplication can be expressed by recursively dividing each matrix into four equal blocks and performing eight smaller matrix multiplications between the blocks.\n",
        "\n",
        "In tomorrow's exercises, you'll have the option to expore batch sizes in more detail.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hd-T_cE3zE1r"
      },
      "source": [
        "## Common Themes in Gradient-Based Optimizers\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hiWAagtzJxM"
      },
      "source": [
        "### Weight Decay\n",
        "\n",
        "Weight decay means that on each iteration, in addition to a regular step, we also shrink each parameter very slightly towards 0 by multiplying a scaling factor close to 1, e.g. 0.9999. Empirically, this seems to help but there are no proofs that apply to deep neural networks.\n",
        "\n",
        "In the case of linear regression, weight decay is mathematically equivalent to having a prior that each parameter is Gaussian distributed - in other words it's very unlikely that the true parameter values are very positive or very negative. This is an example of \"**inductive bias**\" - we make an assumption that helps us in the case where it's justified, and hurts us in the case where it's not justified.\n",
        "\n",
        "For a `Linear` layer, it's common practice to apply weight decay only to the weight and not the bias. It's also common to not apply weight decay to the parameters of a batch normalization layer. Again, there is empirical evidence (such as [Jai et al 2018](https://arxiv.org/pdf/1807.11205.pdf)) and there are heuristic arguments to justify these choices, but no rigorous proofs. Note that PyTorch will implement weight decay on the weights *and* biases of linear layers by default - see the bonus exercises tomorrow for more on this.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3JYcT79zF9O"
      },
      "source": [
        "### Momentum\n",
        "\n",
        "Momentum means that the step includes a term proportional to a moving average of past gradients. [Distill.pub](https://distill.pub/2017/momentum/) has a great article on momentum, which you should definitely read if you have time. Don't worry if you don't understand all of it; skimming parts of it can be very informative. For instance, the first half discusses the **conditioning number** (a very important concept to understand in optimisation), and concludes by giving an intuitive argument for why we generally set the momentum parameter close to 1 for ill-conditioned problems (those with a very large conditioning number).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ry2jHbA4zHI-"
      },
      "source": [
        "## Visualising optimization with pathological curvatures\n",
        "\n",
        "A pathological curvature is a type of surface that is similar to ravines and is particularly tricky for plain SGD optimization. In words, pathological curvatures typically have a steep gradient in one direction with an optimum at the center, while in a second direction we have a slower gradient towards a (global) optimum. Let‚Äôs first create an example surface of this and visualize it:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Z8WbNR3zL0O"
      },
      "outputs": [],
      "source": [
        "def pathological_curve_loss(x: t.Tensor, y: t.Tensor):\n",
        "    # Example of a pathological curvature. There are many more possible, feel free to experiment here!\n",
        "    x_loss = t.tanh(x) ** 2 + 0.01 * t.abs(x)\n",
        "    y_loss = t.sigmoid(y)\n",
        "    return x_loss + y_loss\n",
        "\n",
        "plot_fn(pathological_curve_loss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6yMFc88BOcT"
      },
      "source": [
        "In terms of optimization, you can image that `x` and `y` are weight parameters, and the curvature represents the loss surface over the space of `x` and `y`. Note that in typical networks, we have many, many more parameters than two, and such curvatures can occur in multi-dimensional spaces as well.\n",
        "\n",
        "Ideally, our optimization algorithm would find the center of the ravine and focuses on optimizing the parameters towards the direction of `y`. However, if we encounter a point along the ridges, the gradient is much greater in `x` than `y`, and we might end up jumping from one side to the other. Due to the large gradients, we would have to reduce our learning rate slowing down learning significantly.\n",
        "\n",
        "To test our algorithms, we can implement a simple function to train two parameters on such a surface.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utdGd655zyPx"
      },
      "source": [
        "### Exercise - implement `opt_fn_with_sgd`\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥üî¥‚ö™‚ö™\n",
        "Importance: üîµüîµüîµüîµ‚ö™\n",
        "\n",
        "You should spend up to 15-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Implement the `opt_fn_with_sgd` function using `torch.optim.SGD`. Starting from `(2.5, 2.5)`, run your function and add the resulting trajectory of `(x, y)` pairs to your contour plot. Did it find the minimum? Play with the learning rate and momentum a bit and see how close you can get within 100 iterations.\n",
        "\n",
        "You'll need to repeat the following loop:\n",
        "\n",
        "* Calculate your output (equivalent to altitude in your loss landscape, at coordinates `(x, y)`)\n",
        "* Call `.backward()` on your output, to propagate gradients (more on this tomorrow!)\n",
        "* Call `.step()` on your optimizer, to update your parameters\n",
        "* Call `.zero_grad()` on your optimizer, to zero out the gradients from the previous step, ready to restart the loop\n",
        "\n",
        "A few gotchas:\n",
        "\n",
        "* `torch.optim.SGD` (and other optimizers you'll use) expect iterables of parameters, rather than a single parameter. So rather than passing in the tensor `xy` as the `params` argument, you need to pass in a length-1 list containing `xy`.\n",
        "* Remember to call `detach()` on your `xy` tensor at each step before you add it to your list of points. This is necessary to remove `xy` it from the computational graph.\n",
        "* An important note here - we're not optimising the parameters of a neural network; we're optimising parameters `(x, y)` which represent coordinates at which we evaluate a function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Rbk5v5tzzf7"
      },
      "outputs": [],
      "source": [
        "def opt_fn_with_sgd(fn: Callable, xy: t.Tensor, lr=0.001, momentum=0.98, n_iters: int = 100):\n",
        "    '''\n",
        "    Optimize the a given function starting from the specified point.\n",
        "\n",
        "    xy: shape (2,). The (x, y) starting point.\n",
        "    n_iters: number of steps.\n",
        "    lr, momentum: parameters passed to the torch.optim.SGD optimizer.\n",
        "\n",
        "    Return: (n_iters, 2). The (x,y) BEFORE each step. So out[0] is the starting point.\n",
        "    '''\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "On3HSh0yz17P"
      },
      "source": [
        "<details>\n",
        "<summary>Help - I'm not sure if my <code>opt_fn_with_sgd</code> is implemented properly.</summary>\n",
        "\n",
        "With a learning rate of `0.02` and momentum of `0.99`, my SGD was able to reach `[ 0.8110, -6.3344]` after 100 iterations.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>Help - I'm getting <code>Can't call numpy() on Tensor that requires grad</code>.</summary>\n",
        "\n",
        "This is a protective mechanism built into PyTorch. The idea is that once you convert your Tensor to NumPy, PyTorch can no longer track gradients, but you might not understand this and expect backprop to work on NumPy arrays.\n",
        "\n",
        "All you need to do to convince PyTorch you're a responsible adult is to call detach() on the tensor first, which returns a view that does not require grad and isn't part of the computation graph.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UkYSQIo80G4-"
      },
      "source": [
        "We've also provided you with a function `plot_fn_with_points`, which plots a function as well as a list of points produced by functions like the one above. It works as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Lzj2KAJBOcU"
      },
      "outputs": [],
      "source": [
        "points = []\n",
        "\n",
        "optimizer_list = [\n",
        "    (optim.SGD, {\"lr\": 0.1, \"momentum\": 0.0}),\n",
        "    (optim.SGD, {\"lr\": 0.02, \"momentum\": 0.99}),\n",
        "]\n",
        "\n",
        "for optimizer_class, params in optimizer_list:\n",
        "    xy = t.tensor([2.5, 2.5], requires_grad=True)\n",
        "    xys = opt_fn_with_sgd(pathological_curve_loss, xy=xy, lr=params['lr'], momentum=params['momentum'])\n",
        "\n",
        "    points.append((xys, optimizer_class, params))\n",
        "\n",
        "plot_fn_with_points(pathological_curve_loss, points=points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EsQ__QF90Olm"
      },
      "source": [
        "## Build Your Own Optimizers\n",
        "\n",
        "Now let's build our own drop-in replacement for these three classes from `torch.optim`. The documentation pages for these algorithms have pseudocode you can use to implement your step method.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "brAVgKdR0U61"
      },
      "source": [
        "> **A warning regarding in-place operations**\n",
        ">\n",
        "> Be careful with expressions like `x = x + y` and `x += y`. They are NOT equivalent in Python.\n",
        ">\n",
        "> - The first one allocates a new `Tensor` of the appropriate size and adds `x` and `y` to it, then rebinds `x` to point to the new variable. The original `x` is not modified.\n",
        "> - The second one modifies the storage referred to by `x` to contain the sum of `x` and `y` - it is an \"in-place\" operation.\n",
        ">     - Another way to write the in-place operation is `x.add_(y)` (the trailing underscore indicates an in-place operation).\n",
        ">     - A third way to write the in-place operation is `torch.add(x, y, out=x)`.\n",
        "> - This is rather subtle, so make sure you are clear on the difference. This isn't specific to PyTorch; the built-in Python `list` follows similar behavior: `x = x + y` allocates a new list, while `x += y` is equivalent to `x.extend(y)`.\n",
        ">\n",
        "> The tricky thing that happens here is that both the optimizer and the `Module` in your model have a reference to the same `Parameter` instance.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PLxD7aI0azO"
      },
      "source": [
        "<details>\n",
        "<summary>Question - do you think we should use in-place operations in our optimizer?</summary>\n",
        "\n",
        "You MUST use in-place operations in your optimizer because we want the model to see the change to the Parameter's storage on the next forward pass. If your optimizer allocates a new tensor, the model won't know anything about the new tensor and will continue to use the old, unmodified version.\n",
        "\n",
        "Note, this observation specifically refers to the parameters. When you're updating non-parameter variables that you're tracking, you should be careful not to accidentally use an in-place operation where you shouldn't!.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfxCeK6_0gJN"
      },
      "source": [
        "### More Tips\n",
        "\n",
        "- Your step function shouldn't modify the gradients. Use the `with torch.inference_mode():` context for this. Fun fact: you can instead use `@torch.inference_mode()` (note the preceding `@`) as a method decorator to do the same thing.\n",
        "- If you create any new tensors, they should be on the same device as the corresponding parameter. Use `torch.zeros_like()` or similar for this.\n",
        "- Be careful not to mix up `Parameter` and `Tensor` types in this step.\n",
        "- The actual PyTorch implementations have an additional feature called parameter groups where you can specify different hyperparameters for each group of parameters. You can ignore this for now; we'll come back to it in the next section.\n",
        "\n",
        "Note, the configurations used during testing will start simple (e.g. all parameters set to zero except `lr`) and gradually move to more complicated ones. This will help you track exactly where in your model the error is coming from.\n",
        "\n",
        "You should also fill in the default PyTorch keyword arguments, where appropriate.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f61cy2wb0iAx"
      },
      "source": [
        "### Exercise - implement SGD\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥üî¥üî¥‚ö™\n",
        "Importance: üîµüîµüîµ‚ö™‚ö™\n",
        "\n",
        "You should spend up to 20-25 minutes on this exercise.\n",
        "\n",
        "This is the first of several exercises like it. The first will probably take the longest.\n",
        "```\n",
        "\n",
        "First, you should implement stochastic gradient descent. It should be like the [PyTorch version](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD), but assume `nesterov=False`, `maximize=False`, and `dampening=0`. These simplifications mean that there are many variables in the pseudocode at that link which you can ignore.\n",
        "\n",
        "A few notes / tips (most of these apply to all the implementations, not just SGD):\n",
        "\n",
        "- The values $\\theta_t$ in this pseudocode represent each of the parameters within the list `params`, and the values $g_t$ represent the gradients.\n",
        "- When you zero the gradients, you'll need to iterate over all parameters in `params` and set the gradient to zero.\n",
        "    - Alternatively, you can set `param.grad` to `None` - this has the same effect, but is generally preferred for storage reasons. The next time `backward` is called, the `grad` attribute will be allocated a value just like it would be if it was initially a tensor of zeros.\n",
        "- Your step function shouldn't modify the gradients. This is why we have the decorator `@torch.inference_mode()` (which is equivalent to using the context manager `with torch.inference_mode():`.\n",
        "    - This is similar to `torch.no_grad`, however `torch.inference_mode` is now generally preferred.\n",
        "- If you create any new tensors, they should be on the same device as the corresponding parameter. Use `torch.zeros_like()` or similar for this.\n",
        "- Be careful not to mix up `Parameter` and `Tensor` types in this step. The parameters in `params` are of type `Parameter`, but each `param.grad` should be of type `Tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MKHcfm8M0jV0"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float,\n",
        "        momentum: float = 0.0,\n",
        "        weight_decay: float = 0.0\n",
        "    ):\n",
        "        '''Implements SGD with momentum.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "\n",
        "        '''\n",
        "        params = list(params) # turn params into a list (because it might be a generator)\n",
        "\n",
        "        self.params = params\n",
        "        self.lr = lr\n",
        "        self.mu = momentum\n",
        "        self.lmda = weight_decay\n",
        "\n",
        "        # Set number of steps to zero, and\n",
        "        self.t = 0\n",
        "        self.gs = [t.zeros_like(p) for p in self.params]\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        '''Zeros all gradients of the parameters in `self.params`.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        '''Performs a single optimization step of the SGD algorithm.\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"SGD(lr={self.lr}, momentum={self.mu}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_sgd(SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgW8nik80llT"
      },
      "source": [
        "The configurations used during testing will start simple (e.g. all parameters set to zero except `lr`) and gradually move to more complicated ones. This will help you track exactly where in your model the error is coming from.\n",
        "\n",
        "If you've having trouble, you can use the following process when implementing your optimisers:\n",
        "\n",
        "1. Take the pseudocode from the PyTorch documentation page, and write out the \"simple version\", i.e. without all of the extra variables which you won't need. (It's good practice to be able to parse pseudocode and figure out what it actually means - during the course we'll be doing a lot more of \"transcribing instructions / formulae from paper into code\"). You'll want pen and paper for this!\n",
        "\n",
        "2. Figure out which extra variables you'll need to track within your class.\n",
        "\n",
        "3. Implement the `step` function using these variables.\n",
        "\n",
        "You can click on the expander below to see what the first two steps look like for the case of SGD (try and have a go at each step before you look).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8y-wFF_0mdw"
      },
      "source": [
        "<details>\n",
        "<summary>STEP 1</summary>\n",
        "\n",
        "In the SGD pseudocode, you'll first notice that we can remove the nesterov section, i.e. we always do $g_t \\leftarrow \\boldsymbol{b}_t$. Then, we can actually remove the variable $\\boldsymbol{b_t}$ altogether (because we only needed it to track values while implementing nesterov). Lastly, we have `maximize=False` and `dampening=0`, which allows us to further simplify. So we get the simplified pseudocode:\n",
        "\n",
        "$\n",
        "\\text {for } t=1 \\text { to } \\ldots \\text { do } \\\\\n",
        "\\quad g_t \\leftarrow \\nabla_\\theta f_t\\left(\\theta_{t-1}\\right) \\\\\n",
        "\\quad \\text {if } \\lambda \\neq 0 \\\\\n",
        "\\quad\\quad g_t \\leftarrow g_t+\\lambda \\theta_{t-1} \\\\\n",
        "\\quad \\text {if } \\mu \\neq 0 \\text{ and } t>1 \\\\\n",
        "\\quad\\quad g_t \\leftarrow \\mu g_{t-1} + g_t \\\\\n",
        "\\quad \\theta_t \\leftarrow \\theta_{t-1} - \\gamma g_t\n",
        "$\n",
        "\n",
        "Note - you might find it helpful to name your variables in the `__init__` step in line with their definitions in the pseudocode, e.g. `self.mu = momentum`. This will make it easier to implement the `step` function.\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>STEP 2</summary>\n",
        "\n",
        "In the formula from STEP 1, $\\theta_t$ represents the parameters themselves, and $g_t$ represents variables which we need to keep track of in order to implement momentum. We need to track $g_t$ in our model, e.g. using a line like:\n",
        "\n",
        "```python\n",
        "self.gs = [t.zeros_like(p) for p in self.params]\n",
        "```\n",
        "\n",
        "We also need to track the variable $t$, because the behavour is different when $t=0$. (Technically we could just as easily not do this, because the behaviour when $t=0$ is just the same as the behaviour when $g_t=0$ and $t>0$. But I've left $t$ in my solutions, to make it more obvious how the `SGD.step` function corrsponds to the pseudocode shown in STEP 1.\n",
        "\n",
        "Now, you should be in a good position to attempt the third step: applying SGD in the `step` function, using this algorithm and these tracked variables.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you feel comfortable with this implementation, you can skim through the remaining ones, since there's diminishing marginal returns to be gained from doing the actual exercises."
      ],
      "metadata": {
        "id": "FsEqqWhsh0v9"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdQDgqfd02EI"
      },
      "source": [
        "### Exercise - implement RMSprop\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥üî¥üî¥‚ö™\n",
        "Importance: üîµüîµ‚ö™‚ö™‚ö™\n",
        "\n",
        "You should spend up to 15-25 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Once you've implemented SGD, you should do RMSprop in a similar way. Although the pseudocode is more complicated and there are more variables you'll have to track, there is no big conceptual difference between the task for RMSprop and SGD.\n",
        "\n",
        "If you want to better understand why RMSprop works, then you can return to some of the readings at the top of this page.\n",
        "\n",
        "[Here](https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html) is a link to the PyTorch version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T4Nc81jq02v3"
      },
      "outputs": [],
      "source": [
        "class RMSprop:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.01,\n",
        "        alpha: float = 0.99,\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "        momentum: float = 0.0,\n",
        "    ):\n",
        "        '''Implements RMSprop.\n",
        "\n",
        "        Like the PyTorch version, but assumes centered=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.RMSprop.html\n",
        "\n",
        "        '''\n",
        "        params = list(params) # turn params into a list (because it might be a generator)\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"RMSprop(lr={self.lr}, eps={self.eps}, momentum={self.mu}, weight_decay={self.lmda}, alpha={self.alpha})\"\n",
        "\n",
        "\n",
        "tests.test_rmsprop(RMSprop)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyaxQ0As1EKU"
      },
      "source": [
        "### Exercise - implement Adam\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥üî¥üî¥‚ö™\n",
        "Importance: üîµüîµüîµ‚ö™‚ö™\n",
        "\n",
        "You should spend up to 15-20 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Next, you'll do the same for Adam. This is a very popular optimizer in deep learning, which empirically often outperforms most others. It combines the heuristics of both momentum (via the $\\beta_1$ parameter), and RMSprop's handling of noisy data by dividing by the $l_2$ norm of gradients (via the $\\beta_2$ parameter).\n",
        "\n",
        "[Here](https://pytorch.org/docs/stable/generated/torch.optim.Adam.html) is a link to the PyTorch version.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVgcTAvr1Ndd"
      },
      "outputs": [],
      "source": [
        "class Adam:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        '''Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.Adam.html\n",
        "        '''\n",
        "        params = list(params) # turn params into a list (because it might be a generator)\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"Adam(lr={self.lr}, beta1={self.beta1}, beta2={self.beta2}, eps={self.eps}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_adam(Adam)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j5aUUHVLBOcY"
      },
      "source": [
        "### Exercise - implement AdamW\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥üî¥üî¥‚ö™\n",
        "Importance: üîµüîµüîµ‚ö™‚ö™\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "Finally, you'll adapt your Adam implementation to implement AdamW. This is a variant of Adam which is designed to work better with decoupled weight decay. You can read more about it [here](https://arxiv.org/abs/1711.05101). If you have time, we strongly recommend reading this paper - it is fairly accessible and forces you to engage with what Adam is actually doing.\n",
        "\n",
        "[Here](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html) is a link to the PyTorch version.\n",
        "\n",
        "<details>\n",
        "<summary>Question - can you see why AdamW is different to Adam with weight decay, from the PyTorch documentation pages?</summary>\n",
        "\n",
        "The answer lies in how the weight decay parameter $\\lambda$ is used. In Adam, weight decay is applied to the gradients (before first and second moments are calculated), whereas in AdamW weight decay is applied to the weights themselves (moving them back towards zero).\n",
        "\n",
        "The way AdamW implements weight decay is now generally seen as the \"correct\" way to do it (at least, it's more correct to use the name \"weight decay\" to describe the `weight_decay` hyperparameter in AdamW than to describe the hyperparameter in Adam).\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yW0qyDgLBOcZ"
      },
      "outputs": [],
      "source": [
        "class AdamW:\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[t.nn.parameter.Parameter],\n",
        "        lr: float = 0.001,\n",
        "        betas: Tuple[float, float] = (0.9, 0.999),\n",
        "        eps: float = 1e-08,\n",
        "        weight_decay: float = 0.0,\n",
        "    ):\n",
        "        '''Implements Adam.\n",
        "\n",
        "        Like the PyTorch version, but assumes amsgrad=False and maximize=False\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html\n",
        "        '''\n",
        "        params = list(params) # turn params into a list (because it might be a generator)\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def __repr__(self) -> str:\n",
        "        return f\"AdamW(lr={self.lr}, beta1={self.beta1}, beta2={self.beta2}, eps={self.eps}, weight_decay={self.lmda})\"\n",
        "\n",
        "\n",
        "tests.test_adamw(AdamW)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w40DXSCp1SZG"
      },
      "source": [
        "## Plotting multiple optimisers\n",
        "\n",
        "Finally, we've provided some code which should allow you to plot more than one of your optimisers at once.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sTeqGla61UYW"
      },
      "source": [
        "### Exercise - implement `opt_fn`\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
        "Importance: üîµüîµüîµ‚ö™‚ö™\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "```\n",
        "\n",
        "First, you should fill in this function. It will be pretty much exactly the same as your `opt_fn_with_sgd` from earlier, the only difference is that this function works when passed an arbitrary optimizer (you should only have to change one line of code from your previous function). The `optimizer_hyperparams` argument is a dictionary which will contain keywords like `lr` and `momentum`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_T1CqhBg1V7I"
      },
      "outputs": [],
      "source": [
        "def opt_fn(fn: Callable, xy: t.Tensor, optimizer_class, optimizer_hyperparams: dict, n_iters: int = 100):\n",
        "    '''Optimize the a given function starting from the specified point.\n",
        "\n",
        "    optimizer_class: one of the optimizers you've defined, either SGD, RMSprop, or Adam\n",
        "    optimzer_kwargs: keyword arguments passed to your optimiser (e.g. lr and weight_decay)\n",
        "    '''\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uLHoAs5P1mKf"
      },
      "source": [
        "Once you've implemented this function, you can use `utils.plot_optimization` to create plots of multiple different optimizers at once. An example of how this should work can be found below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbgPKAeBBOca"
      },
      "outputs": [],
      "source": [
        "points = []\n",
        "\n",
        "optimizer_list = [\n",
        "    (SGD, {\"lr\": 0.03, \"momentum\": 0.99}),\n",
        "    (RMSprop, {\"lr\": 0.02, \"alpha\": 0.99, \"momentum\": 0.8}),\n",
        "    (Adam, {\"lr\": 0.2, \"betas\": (0.99, 0.99), \"weight_decay\": 0.005}),\n",
        "]\n",
        "\n",
        "for optimizer_class, params in optimizer_list:\n",
        "    xy = t.tensor([2.5, 2.5], requires_grad=True)\n",
        "    xys = opt_fn(pathological_curve_loss, xy=xy, optimizer_class=optimizer_class, optimizer_hyperparams=params)\n",
        "    points.append((xys, optimizer_class, params))\n",
        "\n",
        "plot_fn_with_points(pathological_curve_loss, points=points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpPeob962DhE"
      },
      "source": [
        "You can try and play around with a few optimisers.\n",
        "\n",
        "* Which ones perform best for this particular function?\n",
        "* Which ones become unstable when you increase their learning rates?\n",
        "* With the same parameters, does `AdamW` beat `Adam`?\n",
        "\n",
        "Here are a few functions you might also want to try out:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_gTHaBsBOca"
      },
      "outputs": [],
      "source": [
        "def bivariate_gaussian(x, y, x_mean=0.0, y_mean=0.0, x_sig=1.0, y_sig=1.0):\n",
        "    norm = 1 / (2 * np.pi * x_sig * y_sig)\n",
        "    x_exp = (-1 * (x - x_mean) ** 2) / (2 * x_sig**2)\n",
        "    y_exp = (-1 * (y - y_mean) ** 2) / (2 * y_sig**2)\n",
        "    return norm * t.exp(x_exp + y_exp)\n",
        "\n",
        "def neg_trimodal_func(x, y):\n",
        "    z = -bivariate_gaussian(x, y, x_mean=1.0, y_mean=-0.5, x_sig=0.2, y_sig=0.2)\n",
        "    z -= bivariate_gaussian(x, y, x_mean=-1.0, y_mean=0.5, x_sig=0.2, y_sig=0.2)\n",
        "    z -= bivariate_gaussian(x, y, x_mean=-0.5, y_mean=-0.8, x_sig=0.2, y_sig=0.2)\n",
        "    return z\n",
        "\n",
        "plot_fn(neg_trimodal_func, x_range=(-2, 2), y_range=(-2, 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sMHGuJ_5BOca"
      },
      "outputs": [],
      "source": [
        "def rosenbrocks_banana_func(x: t.Tensor, y: t.Tensor, a=1, b=100) -> t.Tensor:\n",
        "    return (a - x) ** 2 + b * (y - x**2) ** 2 + 1\n",
        "\n",
        "plot_fn(rosenbrocks_banana_func, x_range=(-2, 2), y_range=(-1, 3), log_scale=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oapL_jP8BOca"
      },
      "source": [
        "<details>\n",
        "<summary>Spoiler - what you should find</summary>\n",
        "\n",
        "For most of these functions, you should find that Adam performs much better. We can increase the learning rate of Adam without losing stability (provided the betas are well-chosen). The Rosenbrocks banana is a well-known exception, which causes most optimizers to perform badly (SGD with very high momentum is pretty much the only thing that works).\n",
        "\n",
        "What is our conclusion here? Should we always use Adam and never look at SGD anymore? The short answer: no. There are many papers saying that in certain situations, SGD (with momentum) generalizes better, where Adam often tends to overfit. Nevertheless, Adam is the most commonly used optimizer in Deep Learning as it usually performs better than other optimizers, especially for deep networks. Furthermore, we usually just stick with the default Adam parameters of `lr=0.001`, `betas=(0.9, 0.999)`, and `eps=1e-8`, which are usually good enough for most problems.\n",
        "</details>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlWO0k4R2Hze"
      },
      "source": [
        "## Bonus - parameter groups\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kGwQPfRC2JCY"
      },
      "source": [
        "> *If you're interested in these exercises then you can go through them, if not then you can move on to the next section (weights and biases).*\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tm-5_QrU2KWo"
      },
      "source": [
        "Rather than passing a single iterable of parameters into an optimizer, you have the option to pass a list of parameter groups, each one with different hyperparameters. As an example of how this might work:\n",
        "\n",
        "```python\n",
        "optim.SGD([\n",
        "    {'params': model.base.parameters()},\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-3}\n",
        "], lr=1e-2, momentum=0.9)\n",
        "```\n",
        "\n",
        "The first argument here is a list of dictionaries, with each dictionary defining a separate parameter group. Each should contain a `params` key, which contains an iterable of parameters belonging to this group. The dictionaries may also contain keyword arguments. If a parameter is not specified in a group, PyTorch uses the value passed as a keyword argument. So the example above is equivalent to:\n",
        "\n",
        "```python\n",
        "optim.SGD([\n",
        "    {'params': model.base.parameters(), 'lr': 1e-2, 'momentum': 0.9},\n",
        "    {'params': model.classifier.parameters(), 'lr': 1e-3, 'momentum': 0.9}\n",
        "])\n",
        "```\n",
        "\n",
        "PyTorch optimisers will store all their params and hyperparams in the `param_groups` attribute, which is a list of dictionaries like the one above, where each one contains *every* hyperparameter rather than just the ones that were specified by the user at initialisation. Optimizers will have this `param_groups` attribute even if they only have one param group - then `param_groups` will just be a list containing a single dictionary.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zAbJ9IcW2NYV"
      },
      "source": [
        "### When to use parameter groups\n",
        "\n",
        "Parameter groups can be useful in several different circumstances. A few examples:\n",
        "\n",
        "* Finetuning a model by freezing earlier layers and only training later layers is an extreme form of parameter grouping. We can use the parameter group syntax to apply a modified form, where the earlier layers have a smaller learning rate. This allows these earlier layers to adapt to the specifics of the problem, while making sure they don't forget all the useful features they've already learned.\n",
        "* Often it's good to treat weights and biases differently, e.g. effects like weight decay are often applied to weights but not biases. PyTorch doesn't differentiate between these two, so you'll have to do this manually using paramter groups.\n",
        "    * This in particular, you might be doing later in the course, if you choose the \"train BERT from scratch\" exercises during the transformers chapter.\n",
        "* On the subject of transformers, weight decay is often *not* applied to embeddings and layernorms in transformer models.\n",
        "\n",
        "More generally, if you're trying to replicate a paper, it's important to be able to use all the same training details that the original authors did, so you can get the same results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0HZotri2PDL"
      },
      "source": [
        "### Exercise - rewrite SGD to use parameter groups\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥üî¥üî¥üî¥\n",
        "Importance: üîµüîµ‚ö™‚ö™‚ö™\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "\n",
        "It's valuable to conceptually understand parameter groups. This exercise has many less important finnicky details, when creating the param groups dictionary.\n",
        "```\n",
        "\n",
        "You should rewrite the `SGD` optimizer from the earlier exercises, to use `param_groups`. A few things to keep in mind during this exercise:\n",
        "\n",
        "* The learning rate must either be specified as a keyword argument, or it must be specified in every group. If it isn't specified as a keyword argument or there's at least one group in which it's not specified, you should raise an error.\n",
        "    * This isn't true for the other hyperparameters like momentum. They all have default values, and so they don't need to be specified.\n",
        "* You should add some code to check that no parameters appear in more than one group (PyTorch raises an error if this happens).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djEjMqKS2QV0"
      },
      "outputs": [],
      "source": [
        "class SGD:\n",
        "\n",
        "    def __init__(self, params, **kwargs):\n",
        "        '''Implements SGD with momentum.\n",
        "\n",
        "        Accepts parameters in groups, or an iterable.\n",
        "\n",
        "        Like the PyTorch version, but assume nesterov=False, maximize=False, and dampening=0\n",
        "            https://pytorch.org/docs/stable/generated/torch.optim.SGD.html#torch.optim.SGD\n",
        "        '''\n",
        "        pass\n",
        "\n",
        "    def zero_grad(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @t.inference_mode()\n",
        "    def step(self) -> None:\n",
        "        pass\n",
        "\n",
        "tests.test_sgd_param_groups(SGD)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k9dJxteKhMtz"
      },
      "source": [
        "# 2Ô∏è‚É£ Weights and Biases\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wylIMmp9KG7e"
      },
      "source": [
        "> ##### Learning Objectives\n",
        ">\n",
        "> * Learn what the most important hyperparameters are, and methods for efficiently searching over hyperparameter space\n",
        "> * Adapt your code from yesterday to log training runs to Weights & Biases, and use this service to run hyperparameter sweeps\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BODPLZgUk8MN"
      },
      "source": [
        "Next, we'll look at methods for choosing hyperparameters effectively. You'll learn how to use **Weights and Biases**, a useful tool for hyperparameter search.\n",
        "\n",
        "The exercises themselves will be based on your ResNet implementations from yesterday, although the principles should carry over to other models you'll build in this course (such as transformers next week).\n",
        "\n",
        "Note, this page only contains a few exercises, and they're relatively short. You're encouraged to spend some time playing around with Weights and Biases, but you should also spend some more time finetuning your ResNet from yesterday (you might want to finetune ResNet during the morning, and look at today's material in the afternoon - you can discuss this with your partner). You should also spend some time reviewing the last three days of material, to make sure there are no large gaps in your understanding.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j20OdUwAk9aP"
      },
      "source": [
        "## CIFAR10\n",
        "\n",
        "The benchmark we'll be training on is [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html), which consists of 60000 32x32 colour images in 10 different classes. Don't peek at what other people online have done for CIFAR10 (it's a common benchmark), because the point is to develop your own process by which you can figure out how to improve your model. Just reading the results of someone else would prevent you from learning how to get the answers. To get an idea of what's possible: using one V100 and a modified ResNet, one entry in the DAWNBench competition was able to achieve 94% test accuracy in 24 epochs and 76 seconds. 94% is approximately [human level performance](http://karpathy.github.io/2011/04/27/manually-classifying-cifar10/).\n",
        "\n",
        "Below is some boilerplate code for downloading and transforming `CIFAR10` data (this shouldn't take more than a minute to run the first time). There are a few differences between this and our code yesterday week - for instance, we omit the `transforms.Resize` from our `transform` object, because CIFAR10 data is already the correct size (unlike the sample images from last week).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nzu3YWRkk_CZ",
        "outputId": "4c146494-cb82-4307-a223-ca9a3ec52b90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 170498071/170498071 [00:13<00:00, 12984289.72it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-2.18.2.min.js\"></script>                <div id=\"f7a4fc53-e50c-4372-a12b-fd8ecd7c0768\" class=\"plotly-graph-div\" style=\"height:600px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"f7a4fc53-e50c-4372-a12b-fd8ecd7c0768\")) {                    Plotly.newPlot(                        \"f7a4fc53-e50c-4372-a12b-fd8ecd7c0768\",                        [{\"name\":\"0\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x11\",\"yaxis\":\"y11\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"1\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x12\",\"yaxis\":\"y12\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"2\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x13\",\"yaxis\":\"y13\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"3\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x14\",\"yaxis\":\"y14\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"4\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x15\",\"yaxis\":\"y15\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"5\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x6\",\"yaxis\":\"y6\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"6\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x7\",\"yaxis\":\"y7\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"7\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x8\",\"yaxis\":\"y8\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"8\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x9\",\"yaxis\":\"y9\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"9\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x10\",\"yaxis\":\"y10\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"10\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x\",\"yaxis\":\"y\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"11\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x2\",\"yaxis\":\"y2\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"12\",\"source\":\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAIAAAD8GO2jAAAIfklEQVR4Xo2VSY9dxRXHzzlVd35Dv7End2O36bYdBxtDICIEJVIUoixYRMoinyGLrPKRssgiShArCAoBkQEwiNhgW4Db7m73+N7rfu++O717azhZdBsIYcF/UaoqnTrDr06p8A/3Ums0ACAiAyAgIgAAMnwXMf+fHYNlq8Bqa0GxlAItEAAAIgAgwGkIsgBn628Tnw4MjGcTAABAQAZmJmIWjJZASkLLeJb214R0duJb9Xj7cZ2MZzEBGBAYUAMyW0Ip6NTqm5We+v+u+t88LLPVmoxBFJKQGA0CA+CXhghACAyP83qcwSmBx36+8npWPwMgMDBbY6pSlRqlK4EEs5GggYHPuAAyCEaDaL+Ca/EsAFhgPBV/LQgDsdEAIJDQqDKtZuD5rkRmBEIm+BomgVClCQp0g8AwMxLjV5dJ3+DHAHhWLAMCE7NbZPEsLzzHlQ5U1kpEAWCILQMQUTw8+ttf/lSv1TYuXwpazajXC2ttw8hoCQCZvmwcgjNUjGiIAJiYmeX4+GBr89MfvfBLSXYqsYkACJaBmVmgMxkd3n7vbZ5VD2+vNJbnzz917YWXfoHoG7TITGe9z3gGHxGYERnJVNnR/v58f8VU8db9jxthJPcf3V1cec6yRWAEAgA22uiy6SEZzga7x9OD4WQYyMa1Z14kjxkMgjxlQ2dkGJjZopC0u/3Ze++88fzzP97ZvDPc376Zl3Lz80+Wlq8TOqfvywrSs/LzWx+Ryvu12tbgADCy8fSt116NnOh7N57SCEjICMayYSOJEImQBLAup5/95993P343jff2d3Ym8VhZI+PRwMymMuhbC4gVk3MyGmzevll3ZdPzjkdDHU/auW118bMP//Hg3q3aXOv6s884gW+JAEFZUxZVkaTp5PjR9p27H75ri2Swt5UkqR+FJK08Od59+OD2pasvIQUOkmB+tLU1mUxWF7uQKWZgo4ssbrVbZTz49OYHrkvj+7f8KApqAVieDI+LJNvd2UmTBFw2Oie0mnTNqxeGrS1kVYz39+6uX3o6SwsdD0hSOjoqq7K0ZjwaxHkahpGUiFyZLO5FjrDlePOTssi1KpkhiGrtemiPH+i8Wr981Xf7aZFvD08mKsVI+3WSVZHsPLzz4It7nujd/+DteuCQ0lrn79/+uFdrFWxMmnb7PaOqLJ105lqmQqgsFDoklr67eH5B6GzPV9NS2aqq16Jz3U67PvfH19/sr/fnlptEzJOTw8P9vdV+75WfvrhS90QxkWQy0JUnFi8+kauqnGlHeI7ns8SKhHLDEmRpWAq35no1KXtzTSIaTcaD4eFkuLfY8Jq+V+Zl4HvSVKJEIRypbeH6biOUizW+0Av9IHDqq9efXrQzqmYzScSqGk0GB6OTMKx5rKAsfOXGJ0NUuecEVaXyKgMZjcej9OTQRUVB1OgExODmGRazZDDaHueJXwtuXFm7vNyRZb4yv/jkYvcH19aMTpJ4VKVTk0/HR/s7Dx8Mjg6nWVJUxSSb7h+Pj6ZZVlVJPtXA0g+yfGp0UW94QQ3l+tWN8SQv4qNPb48+GAycovj97377q0Y013knGx1Egy82arNNH3Z3tsXKeaW5ZEqnSZFhLXBI+EmuTyajrFKTbOZq2NzeXek0HUeUxkoSrLXsLLT78z2wOI3Hw+lBsjfeORgsdZde/snPHt366GT/FvXmFrut+5v3tAUNmBYFSqqA46IsjkYCKSljGXoYOONpkqVFWWRLvVqujBe4QpIE1AwVowlqOL/cDyhU1qTxGNl57ue//uLOfKlK9+Z2UAsYcRJPtNWn3yIwS6WQMOgGN354rdfuvv3X9w8fDfdOOJ2VSmDUCa0ASVZXpnQ8yrNUsxW+++prf76xNj8YxP0rLwWt+Q//9dbOaBTWo7K0Uehr0J35DgkhpOMKsby8cO7qQnex4aGcTJI3Bu8qK5IS+08s9Ffb6FYyzZN8lqOANCuApXH062/+/eDe0iAt7J1NDbosY7cdVIfjPDUF695q+5XfvIw+kgiqRC90W4WYFioNg3D9ysV/vnOzTBzyg42rl/rtdqESKR3JubUGEMnxKQiC9e9vrLWXaTqYUDXf6YadCyqfjfeT5GSiLcdxmswy4UJVTdE4R7HW7gwFjLPCSA7rYTzIjIXxaMJqWRghdVnWglBKObPaKEMkW91WUkwvXl81jcAjMc5TJ2w2l/r7W/FKf+EgPjzYP+55NQu62QyFIBnWDBvPDR3fO3fx3N7m52Bpd+egKC87kUcMEISh6/naaAAjXRE2ovbifK3TY8dVFoXj57rsnOs7dXn9+qXOXEtVttvp9PrtuVbkRkK4rhdGLNivB09eWas1g1a3joCGKZqbk5rAEEopXM8ps8wP/Xa/45cgHI+VCfxAWK2UPnd+Yet8tznvX72+EUZBvdHIZ0lVzYzVSA1juMji0A+Cmrt0obv6xPL+7uFwFIcLkRSBm5vKk1hrNgSwMgodypMksq7vAagZse23mzoUV5/dEC6stVZ2hofxeOx4ripLbWah1zDa1IMI2UaRt3yxt7ren2bxdJrkRSHJgXJW6VwZzxG+RLLCETKcm2nlOh5KFAYdctDhjacugDGgMecMK9tshMd5oSomY4TRjpAAHEZB1HS7883llXapSg9BAmtEVlqXlRECpZQGrUKqlCq0NsZGUaiUkkJ4dc9aC9qeW1vwA5ccCCLf8b0iT7XWkiJCS8JZWOqEobd2cWUwHHoOSaMVGwZrilkFxIRERNqYtJgppYChPqvVwigKQymdmSo911FGG6tIQ1APInRnhVRKEaHr+ghy9cKyMSaoh4v+Aggr0WqJAhBH4xMQWG80BNDxeJJkMyml47jTNGNrla4azeasKrVV2lastes7HlnPFWwFCWuM1VYxVAxYaUvCkY7UUP4Xn54Ah/hhOPsAAAAASUVORK5CYII=\",\"type\":\"image\",\"xaxis\":\"x3\",\"yaxis\":\"y3\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"13\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x4\",\"yaxis\":\"y4\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"},{\"name\":\"14\",\"source\":\"data:image/png;base64,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\",\"type\":\"image\",\"xaxis\":\"x5\",\"yaxis\":\"y5\",\"hovertemplate\":\"x: %{x}<br>y: %{y}<br>color: %{z}<extra></extra>\"}],                        {\"template\":{\"data\":{\"histogram2dcontour\":[{\"type\":\"histogram2dcontour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"choropleth\":[{\"type\":\"choropleth\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"histogram2d\":[{\"type\":\"histogram2d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmap\":[{\"type\":\"heatmap\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"heatmapgl\":[{\"type\":\"heatmapgl\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"contourcarpet\":[{\"type\":\"contourcarpet\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"contour\":[{\"type\":\"contour\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"surface\":[{\"type\":\"surface\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]}],\"mesh3d\":[{\"type\":\"mesh3d\",\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"parcoords\":[{\"type\":\"parcoords\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolargl\":[{\"type\":\"scatterpolargl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"scattergeo\":[{\"type\":\"scattergeo\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterpolar\":[{\"type\":\"scatterpolar\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"scattergl\":[{\"type\":\"scattergl\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatter3d\":[{\"type\":\"scatter3d\",\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattermapbox\":[{\"type\":\"scattermapbox\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scatterternary\":[{\"type\":\"scatterternary\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"scattercarpet\":[{\"type\":\"scattercarpet\",\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}}}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"minorgridcolor\":\"white\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}],\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"#E5ECF6\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}]},\"layout\":{\"autotypenumbers\":\"strict\",\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"hovermode\":\"closest\",\"hoverlabel\":{\"align\":\"left\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"#E5ECF6\",\"polar\":{\"bgcolor\":\"#E5ECF6\",\"angularaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"radialaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"ternary\":{\"bgcolor\":\"#E5ECF6\",\"aaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"},\"caxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\"}},\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]]},\"xaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"yaxis\":{\"gridcolor\":\"white\",\"linecolor\":\"white\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"white\",\"automargin\":true,\"zerolinewidth\":2},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"yaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2},\"zaxis\":{\"backgroundcolor\":\"#E5ECF6\",\"gridcolor\":\"white\",\"linecolor\":\"white\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"white\",\"gridwidth\":2}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"geo\":{\"bgcolor\":\"white\",\"landcolor\":\"#E5ECF6\",\"subunitcolor\":\"white\",\"showland\":true,\"showlakes\":true,\"lakecolor\":\"white\"},\"title\":{\"x\":0.05},\"mapbox\":{\"style\":\"light\"}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.18400000000000002]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.0,0.2866666666666666]},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.20400000000000001,0.388],\"matches\":\"x\"},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.0,0.2866666666666666],\"matches\":\"y\",\"showticklabels\":false},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.40800000000000003,0.5920000000000001],\"matches\":\"x\"},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.2866666666666666],\"matches\":\"y\",\"showticklabels\":false},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.6120000000000001,0.7960000000000002],\"matches\":\"x\"},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.2866666666666666],\"matches\":\"y\",\"showticklabels\":false},\"xaxis5\":{\"anchor\":\"y5\",\"domain\":[0.8160000000000001,1.0],\"matches\":\"x\"},\"yaxis5\":{\"anchor\":\"x5\",\"domain\":[0.0,0.2866666666666666],\"matches\":\"y\",\"showticklabels\":false},\"xaxis6\":{\"anchor\":\"y6\",\"domain\":[0.0,0.18400000000000002],\"matches\":\"x\",\"showticklabels\":false},\"yaxis6\":{\"anchor\":\"x6\",\"domain\":[0.35666666666666663,0.6433333333333333],\"matches\":\"y\"},\"xaxis7\":{\"anchor\":\"y7\",\"domain\":[0.20400000000000001,0.388],\"matches\":\"x\",\"showticklabels\":false},\"yaxis7\":{\"anchor\":\"x7\",\"domain\":[0.35666666666666663,0.6433333333333333],\"matches\":\"y\",\"showticklabels\":false},\"xaxis8\":{\"anchor\":\"y8\",\"domain\":[0.40800000000000003,0.5920000000000001],\"matches\":\"x\",\"showticklabels\":false},\"yaxis8\":{\"anchor\":\"x8\",\"domain\":[0.35666666666666663,0.6433333333333333],\"matches\":\"y\",\"showticklabels\":false},\"xaxis9\":{\"anchor\":\"y9\",\"domain\":[0.6120000000000001,0.7960000000000002],\"matches\":\"x\",\"showticklabels\":false},\"yaxis9\":{\"anchor\":\"x9\",\"domain\":[0.35666666666666663,0.6433333333333333],\"matches\":\"y\",\"showticklabels\":false},\"xaxis10\":{\"anchor\":\"y10\",\"domain\":[0.8160000000000001,1.0],\"matches\":\"x\",\"showticklabels\":false},\"yaxis10\":{\"anchor\":\"x10\",\"domain\":[0.35666666666666663,0.6433333333333333],\"matches\":\"y\",\"showticklabels\":false},\"xaxis11\":{\"anchor\":\"y11\",\"domain\":[0.0,0.18400000000000002],\"matches\":\"x\",\"showticklabels\":false},\"yaxis11\":{\"anchor\":\"x11\",\"domain\":[0.7133333333333333,0.9999999999999999],\"matches\":\"y\"},\"xaxis12\":{\"anchor\":\"y12\",\"domain\":[0.20400000000000001,0.388],\"matches\":\"x\",\"showticklabels\":false},\"yaxis12\":{\"anchor\":\"x12\",\"domain\":[0.7133333333333333,0.9999999999999999],\"matches\":\"y\",\"showticklabels\":false},\"xaxis13\":{\"anchor\":\"y13\",\"domain\":[0.40800000000000003,0.5920000000000001],\"matches\":\"x\",\"showticklabels\":false},\"yaxis13\":{\"anchor\":\"x13\",\"domain\":[0.7133333333333333,0.9999999999999999],\"matches\":\"y\",\"showticklabels\":false},\"xaxis14\":{\"anchor\":\"y14\",\"domain\":[0.6120000000000001,0.7960000000000002],\"matches\":\"x\",\"showticklabels\":false},\"yaxis14\":{\"anchor\":\"x14\",\"domain\":[0.7133333333333333,0.9999999999999999],\"matches\":\"y\",\"showticklabels\":false},\"xaxis15\":{\"anchor\":\"y15\",\"domain\":[0.8160000000000001,1.0],\"matches\":\"x\",\"showticklabels\":false},\"yaxis15\":{\"anchor\":\"x15\",\"domain\":[0.7133333333333333,0.9999999999999999],\"matches\":\"y\",\"showticklabels\":false},\"annotations\":[{\"font\":{},\"showarrow\":false,\"text\":\"deer\",\"x\":0.09200000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2866666666666666,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"horse\",\"x\":0.29600000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2866666666666666,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"horse\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2866666666666666,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"bird\",\"x\":0.7040000000000002,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2866666666666666,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"truck\",\"x\":0.908,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.2866666666666666,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"automobile\",\"x\":0.09200000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6433333333333333,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"bird\",\"x\":0.29600000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6433333333333333,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"horse\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6433333333333333,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"ship\",\"x\":0.7040000000000002,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6433333333333333,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"cat\",\"x\":0.908,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.6433333333333333,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"frog\",\"x\":0.09200000000000001,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"truck\",\"x\":0.29600000000000004,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"truck\",\"x\":0.5,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"deer\",\"x\":0.7040000000000002,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{},\"showarrow\":false,\"text\":\"automobile\",\"x\":0.908,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.9999999999999999,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"CIFAR-10 images\"},\"height\":600},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('f7a4fc53-e50c-4372-a12b-fd8ecd7c0768');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_cifar(subset: int = 1):\n",
        "    cifar_trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=IMAGENET_TRANSFORM)\n",
        "    cifar_testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=IMAGENET_TRANSFORM)\n",
        "    if subset > 1:\n",
        "        cifar_trainset = Subset(cifar_trainset, indices=range(0, len(cifar_trainset), subset))\n",
        "        cifar_testset = Subset(cifar_testset, indices=range(0, len(cifar_testset), subset))\n",
        "    return cifar_trainset, cifar_testset\n",
        "\n",
        "cifar_trainset, cifar_testset = get_cifar()\n",
        "\n",
        "imshow(\n",
        "    cifar_trainset.data[:15],\n",
        "    facet_col=0,\n",
        "    facet_col_wrap=5,\n",
        "    facet_labels=[cifar_trainset.classes[i] for i in cifar_trainset.targets[:15]],\n",
        "    title=\"CIFAR-10 images\",\n",
        "    height=600\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZZ_7GHpnYiB"
      },
      "source": [
        "We have also provided a basic training & testing loop, almost identical to the one you used yesterday. This one doesn't use `wandb` at all, although it does plot the train loss and test accuracy when the function finishes running. You should run this function to verify your model is working, and that the loss is going down. Also, make sure you understand what each part of this function is doing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_9f9_ronhDY"
      },
      "source": [
        "## Train function (modular)\n",
        "\n",
        "First, let's build on the training function we used yesterday. Previously, we just used a single `train` function which took a dataclass as argument. But this resulted in a very long function with many nested loops and some repeated code. Instead, we'll split this function into several smaller functions, each of which will be responsible for a single part of the training process. This will make our code more modular, and easier to read and debug. We'll also wrap all of these functions into a class, which will make it easier to pass around the data and hyperparameters.\n",
        "\n",
        "First, we define our dataclass:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4OfIPWYKG7f"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ResNetTrainingArgs():\n",
        "    batch_size: int = 64\n",
        "    epochs: int = 3\n",
        "    learning_rate: float = 1e-3\n",
        "    n_classes: int = 10\n",
        "    subset: int = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xo4Xvi98KG7f"
      },
      "source": [
        "Next, we define our `ResNetTrainer` class. You should read through this yourself, to make sure you understand the role of each method. A brief summary:\n",
        "\n",
        "* In `__init__`, we define our model, optimizer, and datasets. We also define a dictionary called `logged_variables`, which we'll use to store the loss and accuracy at each step.\n",
        "* The `to_device` method is a helper function, to move our data to the right device (this is a common cause of errors!).\n",
        "* The `training_step` method performs a single gradient update step on a single batch of data, and returns the loss.\n",
        "* The `validation_step` method performs a single evaluation of a batch of data, and returns number of correct classifications.\n",
        "    * Note the use of the decorator `torch.inference_mode()` for this method, which stops gradients propagating.\n",
        "* The `train` method repeatedly calls `training_step` and `validation_step`, and logs the loss and accuracy at each step.\n",
        "    * Note that we've called `model.train()` and `model.eval()` before the training and evaluation steps respectively. This is important because the model has BatchNorm layers, which behave differently in training vs eval mode (in our simple MLP, we didn't have this problem).\n",
        "\n",
        "Note that the optimizer has been defined to only update the parameters in the final layer of the model. This is a common strategy for fine-tuning pretrained models (especially image classifiers). The term for this is **feature extraction**. We've also used the function `get_resnet_for_feature_extraction`, which returns a version of the `ResNet34` model, but with this final layer replaced with a randomly initialized linear layer, with number of output features equal to `args.n_classes`. For more on this, see the bonus section of yesterday's exercises."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "katLUR-IKG7f"
      },
      "outputs": [],
      "source": [
        "class ResNetTrainer:\n",
        "\tdef __init__(self, args: ResNetTrainingArgs):\n",
        "\t\tself.args = args\n",
        "\t\tself.model = get_resnet_for_feature_extraction(args.n_classes).to(device)\n",
        "\t\tself.optimizer = t.optim.Adam(self.model.out_layers[-1].parameters(), lr=args.learning_rate)\n",
        "\t\tself.trainset, self.testset = get_cifar(subset=args.subset)\n",
        "\t\tself.logged_variables = {\"loss\": [], \"accuracy\": []}\n",
        "\n",
        "\tdef to_device(self, *args):\n",
        "\t\treturn [x.to(device) for x in args]\n",
        "\n",
        "\tdef training_step(self, imgs: Tensor, labels: Tensor) -> t.Tensor:\n",
        "\t\timgs, labels = self.to_device(imgs, labels)\n",
        "\t\tlogits = self.model(imgs)\n",
        "\t\tloss = F.cross_entropy(logits, labels)\n",
        "\t\tloss.backward()\n",
        "\t\tself.optimizer.step()\n",
        "\t\tself.optimizer.zero_grad()\n",
        "\t\treturn loss\n",
        "\n",
        "\t@t.inference_mode()\n",
        "\tdef validation_step(self, imgs: Tensor, labels: Tensor) -> t.Tensor:\n",
        "\t\timgs, labels = self.to_device(imgs, labels)\n",
        "\t\tlogits = self.model(imgs)\n",
        "\t\treturn (logits.argmax(dim=1) == labels).sum()\n",
        "\n",
        "\tdef train(self):\n",
        "\n",
        "\t\tfor epoch in range(self.args.epochs):\n",
        "\n",
        "\t\t\t# Load data\n",
        "\t\t\ttrain_dataloader = DataLoader(self.trainset, batch_size=self.args.batch_size, shuffle=True)\n",
        "\t\t\tval_dataloader = DataLoader(self.testset, batch_size=self.args.batch_size, shuffle=True)\n",
        "\t\t\tprogress_bar = tqdm(total=len(train_dataloader))\n",
        "\n",
        "\t\t\t# Training loop (includes updating progress bar, and logging loss)\n",
        "\t\t\tself.model.train()\n",
        "\t\t\tfor imgs, labels in train_dataloader:\n",
        "\t\t\t\tloss = self.training_step(imgs, labels)\n",
        "\t\t\t\tself.logged_variables[\"loss\"].append(loss.item())\n",
        "\t\t\t\tprogress_bar.update()\n",
        "\t\t\t\tprogress_bar.set_description(f\"Epoch {epoch+1}/{self.args.epochs}, Loss = {loss:.2f}\")\n",
        "\n",
        "\t\t\t# Compute accuracy by summing n_correct over all batches, and dividing by number of items\n",
        "\t\t\tself.model.eval()\n",
        "\t\t\taccuracy = sum(self.validation_step(imgs, labels) for imgs, labels in val_dataloader) / len(self.testset)\n",
        "\n",
        "\t\t\t# Update progress bar description to include accuracy, and log accuracy\n",
        "\t\t\tprogress_bar.set_description(f\"Epoch {epoch+1}/{self.args.epochs}, Loss = {loss:.2f}, Accuracy = {accuracy:.2f}\")\n",
        "\t\t\tself.logged_variables[\"accuracy\"].append(accuracy.item())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With this class, we can perform feature extraction on our model as follows:\n"
      ],
      "metadata": {
        "id": "ZqvKwMmGTUea"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "args = ResNetTrainingArgs()\n",
        "trainer = ResNetTrainer(args)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "kIYS_JLtTWPh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_train_loss_and_test_accuracy_from_trainer(trainer, title=\"Feature extraction with ResNet34\")"
      ],
      "metadata": {
        "id": "37CETPHuUhod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's see how well our ResNet performs on the first few inputs!\n"
      ],
      "metadata": {
        "id": "ekBjCMU_TUaD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test_resnet_on_random_input(model: ResNet34, n_inputs: int = 3):\n",
        "    indices = np.random.choice(len(cifar_trainset), n_inputs).tolist()\n",
        "    classes = [cifar_trainset.classes[cifar_trainset.targets[i]] for i in indices]\n",
        "    imgs = cifar_trainset.data[indices]\n",
        "    device = next(model.parameters()).device\n",
        "    with t.inference_mode():\n",
        "        x = t.stack(list(map(IMAGENET_TRANSFORM, imgs)))\n",
        "        logits: t.Tensor = model(x.to(device))\n",
        "    probs = logits.softmax(-1)\n",
        "    if probs.ndim == 1: probs = probs.unsqueeze(0)\n",
        "    for img, label, prob in zip(imgs, classes, probs):\n",
        "        display(HTML(f\"<h2>Classification probabilities (true class = {label})</h2>\"))\n",
        "        imshow(\n",
        "            img,\n",
        "            width=200, height=200, margin=0,\n",
        "            xaxis_visible=False, yaxis_visible=False\n",
        "        )\n",
        "        bar(\n",
        "            prob,\n",
        "            x=cifar_trainset.classes,\n",
        "            template=\"ggplot2\", width=600, height=400,\n",
        "            labels={\"x\": \"Classification\", \"y\": \"Probability\"},\n",
        "            text_auto='.2f', showlegend=False,\n",
        "        )\n",
        "\n",
        "\n",
        "test_resnet_on_random_input(trainer.model)"
      ],
      "metadata": {
        "id": "sPcu_UKkTZgA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VrTJ3_mBKG7g"
      },
      "source": [
        "## What is Weights and Biases?\n",
        "\n",
        "Weights and Biases is a cloud service that allows you to log data from experiments. Your logged data is shown in graphs during training, and you can easily compare logs across different runs. It also allows you to run **sweeps**, where you can specifiy a distribution over hyperparameters and then start a sequence of test runs which use hyperparameters sampled from this distribution.\n",
        "\n",
        "Before you run any of the code below, you should visit the [Weights and Biases homepage](https://wandb.ai/home), and create your own account.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bfZQBuMiKG7g"
      },
      "outputs": [],
      "source": [
        "import wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJ7qUgJ5KG7g"
      },
      "source": [
        "We'll be able to keep the same structure of training loop when using weights and biases, we'll just have to add a few functions. The key functions to know are:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `wandb.init`\n",
        "\n",
        "This initialises a training run. It should be called once, at the start of your training loop.\n",
        "\n",
        "A few important arguments are:\n",
        "\n",
        "* `project` - the name of the project where you're sending the new run. For example, this could be `'day3-resnet'` for us. You can have many different runs in each project.\n",
        "* `name` - a display name for this run. By default, if this isn't supplied, wandb generates a random 2-word name for you (e.g. `gentle-sunflower-42`).\n",
        "* `config` - a dictionary containing hyperparameters for this run. If you pass this dictionary, then you can compare different runs' hyperparameters & results in a single table. Alternatively, you can pass a dataclass.\n",
        "\n",
        "For these first two, we'll create a new dataclass (which inherits from our previous one, so gets all the same data plus this new data):"
      ],
      "metadata": {
        "id": "N0PBSgy881k1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ResNetTrainingArgsWandb(ResNetTrainingArgs):\n",
        "    wandb_project: Optional[str] = 'day3-resnet'\n",
        "    wandb_name: Optional[str] = None"
      ],
      "metadata": {
        "id": "-ecwJA9h9E6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `wandb.watch`\n",
        "\n",
        "This function tells wandb to watch a model. This means that it will log the gradients and parameters of the model during training. We'll call this function once, after we've created our model.\n",
        "\n",
        "The first argument to this function is your model (or a list of models). Another two important arguments:\n",
        "\n",
        "* `log`, which can take the value `'gradients'`, `'parameters'`, or `'all'`, and which determines what gets tracked. Default is `'gradients'`.\n",
        "* `log_freq`, which is an integer. Logging happens once every `log_freq` batches. Default is 1000."
      ],
      "metadata": {
        "id": "ZtjGF2FZ85im"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `wandb.log`\n",
        "\n",
        "For logging metrics to the wandb dashboard. This is used as `wandb.log(data, step)`, where `step` is an integer (the x-axis on your metric plots) and `data` is a dictionary of metrics (i.e. the keys are metric names, and the values are metric values)."
      ],
      "metadata": {
        "id": "nK-tJLIo9KtY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### `wandb.finish`\n",
        "\n",
        "This function should be called at the end of your training loop. It finishes the run, and saves the results to the cloud. If you terminate a run early, remember to still call this."
      ],
      "metadata": {
        "id": "CZ-8Uipn9L4e"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - rewrite training loop\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
        "Importance: üîµüîµüîµüîµ‚ö™\n",
        "\n",
        "You should spend up to 10-20 minutes on this exercise.\n",
        "```"
      ],
      "metadata": {
        "id": "rW74ybbJ9QNB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You should now take the training loop from above (or your own implementation - whichever you prefer) and rewrite it to use the four `wandb` functions above.\n",
        "\n",
        "A few notes:\n",
        "\n",
        "* You can get rid of the dictionary for logging variables, because you'll be using `wandb.log` instead.\n",
        "* It's often useful to have a global `step` variable in your training code, which keeps track of the number of update steps which have taken place. You can pass this step argument to `wandb.log`. Alternatively, you can just omit the `step` argument.\n",
        "* If you use `wandb.watch`, you'll need to decrease the `log_freq` value (since your training by default has less than 1000 batches). You'll also want to log just the parameters that are changing (remember that most of them are frozen). See yesterday's code for how to do this (specifically, the `get_resnet_for_feature_extraction` function).\n",
        "\n"
      ],
      "metadata": {
        "id": "fflJ09Cx9RYD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# YOUR CODE HERE - write `ResNetTrainerWandb` class\n",
        "\n",
        "\n",
        "args = ResNetTrainingArgsWandb()\n",
        "trainer = ResNetTrainerWandb(args)\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "zKs89vwB9T3R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run the code for the first time, you'll have to login to Weights and Biases, and paste an API key into VSCode. After this is done, your Weights and Biases training run will start. It'll give you a lot of output text, one line of which will look like:\n",
        "\n",
        "```\n",
        "View run at https://wandb.ai/<USERNAME>/<PROJECT-NAME>/runs/<RUN-NAME>\n",
        "```\n",
        "\n",
        "which you can click on to visit the run page.\n",
        "\n",
        "A nice thing about using Weights and Biases is that you don't need to worry about generating your own plots, that will all be done for you when you visit the page."
      ],
      "metadata": {
        "id": "AYeQvLDnEdxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run & project pages\n",
        "\n",
        "The page you visit will show you a plot of all the variables you've logged, among other things. You can do many things with these plots (e.g. click on the \"edit\" icon for your `train_loss` plot, and apply smoothing to get a better picture of your loss curve).\n",
        "\n",
        "The charts are a useful feature of the run page that gets opened when you click on the run page link, but they're not the only feature. You can also navigate to the project page (click on the option to the right of **Projects** on the bar at the top of the Wandb page), and see superimposed plots of all the runs in this project. You can also click on the **Table** icon on the left hand sidebar to see a table of all the runs in this project, which contains useful information (e.g. runtime, the most recent values of any logged variables, etc). However, comparing runs like this becomes especially useful when we start doing hyperparameter search."
      ],
      "metadata": {
        "id": "PdPdWUIr9wBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Hyperparameter search\n",
        "\n",
        "One way to search for good hyperparameters is to choose a set of values for each hyperparameter, and then search all combinations of those specific values. This is called **grid search**. The values don't need to be evenly spaced and you can incorporate any knowledge you have about plausible values from similar problems to choose the set of values. Searching the product of sets takes exponential time, so is really only feasible if there are a small number of hyperparameters. I would recommend forgetting about grid search if you have more than 3 hyperparameters, which in deep learning is \"always\".\n",
        "\n",
        "A much better idea is for each hyperparameter, decide on a sampling distribution and then on each trial just sample a random value from that distribution. This is called **random search** and back in 2012, you could get a [publication](https://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) for this. The diagram below shows the main reason that random search outperforms grid search. Empirically, some hyperparameters matter more than others, and random search benefits from having tried more distinct values in the important dimensions, increasing the chances of finding a \"peak\" between the grid points.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/callummcdougall/Fundamentals/main/images/grid_vs_random.png\" width=\"540\">\n",
        "\n",
        "\n",
        "It's worth noting that both of these searches are vastly less efficient than gradient descent at finding optima - imagine if you could only train neural networks by randomly initializing them and checking the loss! Either of these search methods without a dose of human (or eventually AI) judgement is just a great way to turn electricity into a bunch of models that don't perform very well."
      ],
      "metadata": {
        "id": "yEVSxRx69xeO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Running hyperparameter sweeps with `wandb`\n",
        "\n",
        "Now we've come to one of the most impressive features of `wandb` - being able to perform hyperparameter sweeps.\n",
        "\n",
        "To perform hyperparameter sweeps, we follow the following 3-step process:\n",
        "\n",
        "1. Define a `sweep_config` dict, which specifies how we'll randomize hyperparameters during our sweep (more on the exact syntax of this below).\n",
        "2. Define a training function. Importantly, this function just take no arguments (i.e. you should be able to call it as `train()`).\n",
        "    * You will be able to access the values inside `sweep_config[\"parameters\"]` dict using `wandb.config`, you should do this inside your `train()` function.\n",
        "3. Run a sweep, using the `wandb.sweep` and `wandb.agent` functions. This will run the training function with different hyperparameters each time."
      ],
      "metadata": {
        "id": "fsizdNJp9z7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Exercise - define a sweep config (step 1)\n",
        "\n",
        "```yaml\n",
        "Difficulty: üî¥üî¥‚ö™‚ö™‚ö™\n",
        "Importance: üîµüîµüîµ‚ö™‚ö™\n",
        "\n",
        "You should spend up to 10-15 minutes on this exercise.\n",
        "\n",
        "Learning how to use wandb for sweeps is very useful, so make sure you understand all parts of this code.\n",
        "```\n",
        "\n",
        "You should define a dictionary `sweep_config`, which sets out the following rules for hyperparameter sweeps:\n",
        "\n",
        "* Hyperparameters are chosen **randomly**, according to the distributions given in the dictionary\n",
        "* Your goal is to **maximize** the **accuracy** metric\n",
        "* The hyperparameters you vary are:\n",
        "    * `learning_rate` - a log-uniform distribution between 1e-4 and 1e-1\n",
        "    * `batch_size` - randomly chosen from (32, 64, 128, 256)\n",
        "    * `epochs` - randomly chosen from (1, 2, 3)\n",
        "\n",
        "*(A note on the log-uniform distribution - this means a random value `X` will be chosen between `min` and `max` s.t. `log(X)` is uniformly distributed between `log(min)` and `log(max)`. Can you see why a log uniform distribution for the learning rate makes more sense than a uniform distribution?)*\n",
        "\n",
        "You can read the syntax for sweep config dictionaries [here](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration)."
      ],
      "metadata": {
        "id": "zQieI_Dh91ZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_config = dict()\n",
        "\n",
        "# YOUR CODE HERE - fill `sweep_config`\n",
        "\n",
        "\n",
        "tests.test_sweep_config(sweep_config)"
      ],
      "metadata": {
        "id": "u-x7fz8p98n6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define a training function (step 2)\n",
        "\n",
        "Now, we have a `train` function. This takes no arguments, and it implements a training loop just like we've seen before. Note that we've set things like `args.batch_size` from the `wandb.config` dictionary, which is how we access the hyperparameters which are set at the start of each sweep.\n",
        "\n",
        "Note, we don't use `super().__init__()` inside our init method here. This is because we need to update `args` *after* initializing weights & biases (see below for an explanation of why), which requires us to rewrite our init function with a different structure.\n",
        "\n",
        "<details>\n",
        "<summary>Question - why do we update <code>args</code> after calling <code>wandb.init()</code>, not before?</summary>\n",
        "\n",
        "Weights & Biases is very particular about the order in which things are defined. In particular, we can't access the `wandb.config` object until we've called `wandb.init()`. The easiest way to get around this is to slightly restructure our trainer's `__init__` method as follows:\n",
        "\n",
        "* Call `wandb.init` first (don't pass a `config` argument),\n",
        "* Then override the values in `args` with the values in `wandb.config`,\n",
        "* Then do the rest of the things in your trainer's `__init__` method (e.g. defining your model and optimizer).\n",
        "\n",
        "</details>\n",
        "\n",
        "<details>\n",
        "<summary>A note on using <code>replace</code>.</summary>\n",
        "\n",
        "We could replace the following code:\n",
        "\n",
        "```python\n",
        "self.args.batch_size = wandb.config[\"batch_size\"]\n",
        "self.args.epochs = wandb.config[\"epochs\"]\n",
        "self.args.learning_rate = wandb.config[\"learning_rate\"]\n",
        "```\n",
        "\n",
        "With a single line that does this all at once:\n",
        "\n",
        "```python\n",
        "self.args = replace(args, **wandb.config)\n",
        "```\n",
        "\n",
        "This will update all the values of `args` which also appear in `wandb.config`. It's a bit less readable, but a nice way of making our code shorter if you prefer this syntax!\n",
        "\n",
        "</details>\n"
      ],
      "metadata": {
        "id": "OptgEHAQ-WhQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (2) Define a training function which takes no arguments, and uses `wandb.config` to get hyperparams\n",
        "\n",
        "class ResNetTrainerWandbSweeps(ResNetTrainerWandb):\n",
        "    '''\n",
        "    New training class made specifically for hyperparameter sweeps, which overrides the values in\n",
        "    `args` with those in `wandb.config` before defining model/optimizer/datasets.\n",
        "    '''\n",
        "    def __init__(self, args: ResNetTrainingArgsWandb):\n",
        "\n",
        "        # Initialize\n",
        "        wandb.init(name=args.wandb_name)\n",
        "\n",
        "        # Update args with the values in wandb.config\n",
        "        self.args.batch_size = wandb.config[\"batch_size\"]\n",
        "        self.args.epochs = wandb.config[\"epochs\"]\n",
        "        self.args.learning_rate = wandb.config[\"learning_rate\"]\n",
        "\n",
        "        # Perform the previous steps (initialize model & other important objects)\n",
        "        self.model = get_resnet_for_feature_extraction(self.args.n_classes).to(device)\n",
        "        self.optimizer = t.optim.Adam(self.model.out_layers[-1].parameters(), lr=self.args.learning_rate)\n",
        "        self.trainset, self.testset = get_cifar(subset=self.args.subset)\n",
        "        self.step = 0\n",
        "        wandb.watch(self.model.out_layers[-1], log=\"all\", log_freq=20)\n",
        "\n",
        "\n",
        "def train():\n",
        "    args = ResNetTrainingArgsWandb()\n",
        "    trainer = ResNetTrainerWandbSweeps(args)\n",
        "    trainer.train()"
      ],
      "metadata": {
        "id": "1gUQlBKh-bQD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run your sweep (step 3)\n",
        "\n",
        "Finally, you can use the code below to run your sweep! This will probably take a while, because you're doing three separate full training and validation runs."
      ],
      "metadata": {
        "id": "eWjykUEj-eQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sweep_id = wandb.sweep(sweep=sweep_config, project='day3-resnet-sweep')\n",
        "wandb.agent(sweep_id=sweep_id, function=train, count=3)\n",
        "wandb.finish()"
      ],
      "metadata": {
        "id": "1xW73Hfl-fNW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you run this code, you should click on the link which looks like:\n",
        "\n",
        "```\n",
        "View sweep at https://wandb.ai/<USERNAME>/<PROJECT-NAME>/sweeps/<SWEEP-NAME>\n",
        "```\n",
        "\n",
        "This link will bring you to a page comparing each of your sweeps. You'll be able to see overlaid graphs of each of their training loss and test accuracy, as well as a bunch of other cool things like:\n",
        "\n",
        "* Bar charts of the [importance](https://docs.wandb.ai/ref/app/features/panels/parameter-importance) (and correlation) of each hyperparameter wrt the target metric. Note that only looking at the correlation could be misleading - something can have a correlation of 1, but still have a very small effect on the metric.\n",
        "* A [parallel coordinates plot](https://docs.wandb.ai/ref/app/features/panels/parallel-coordinates), which summarises the relationship between the hyperparameters in your config and the model metric you're optimising.\n",
        "\n",
        "What can you infer from these results? Are there any hyperparameters which are especially correlated / anticorrelated with the target metric? Are there any results which suggest the model is being undertrained?\n"
      ],
      "metadata": {
        "id": "xN0lnvSJ-hcY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "<details>\n",
        "<summary>Note on using YAML files (optional)</summary>\n",
        "\n",
        "Rather than using a dictionary, you can alternatively store the `sweep_config` data in a YAML file if you prefer. You will then be able to run a sweep via the following terminal commands:\n",
        "\n",
        "```\n",
        "wandb sweep sweep_config.yaml\n",
        "\n",
        "wandb agent <SWEEP_ID>\n",
        "```\n",
        "\n",
        "where `SWEEP_ID` is the value returned from the first terminal command. You will also need to add another line to the YAML file, specifying the program to be run. For instance, your YAML file might start like this:\n",
        "\n",
        "```yaml\n",
        "program: train.py\n",
        "method: random\n",
        "metric:\n",
        "    name: test_accuracy\n",
        "    goal: maximize\n",
        "```\n",
        "\n",
        "For more, see [this link](https://docs.wandb.ai/guides/sweeps/define-sweep-configuration).\n",
        "\n",
        "</details>"
      ],
      "metadata": {
        "id": "J2op0soHAryv"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wwTcPbIKG7j"
      },
      "source": [
        "# 3Ô∏è‚É£ Bonus\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYJ8iliOKG7j"
      },
      "source": [
        "Congratulations for getting to the end of the main content! This section gives some suggestions for more features of Weights and Biases to explore, or some other experiments you can run.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkqD2h9gKG7j"
      },
      "source": [
        "## Scaling Laws\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxsQW0dcKG7j"
      },
      "source": [
        "These bonus exercises are taken directly from Jacob Hilton's [online deep learning curriculum](https://github.com/jacobhilton/deep_learning_curriculum/blob/master/2-Scaling-Laws.md) (which is what the original version of the ARENA course was based on).\n",
        "\n",
        "First, you can start by reading the [Chinchilla paper](https://arxiv.org/abs/2203.15556). This is a correction to the original scaling laws paper: parameter count scales linearly with token budget for compute-optimal models, not ~quadratically. The difference comes from using a separately-tuned learning rate schedule for each token budget, rather than using a single training run to measure performance for every token budget. This highlights the importance of hyperparameter tuning for measuring scaling law exponents.\n",
        "\n",
        "You don't have to read the entire paper, just skim the graphs. Don't worry if they don't all make sense yet (it will be more illuminating when we study LLMs next week). Note that, although it specifically applies to language models, the key underlying ideas of tradeoffs between optimal dataset size and model size are generally applicable.\n",
        "\n",
        "### Suggested exercise\n",
        "\n",
        "Perform your own study of scaling laws for MNIST.\n",
        "\n",
        "- Write a script to train a small CNN on MNIST, or find one you have written previously.\n",
        "- Training for a single epoch only, vary the model size and dataset size. For the model size, multiply the width by powers of sqrt(2) (rounding if necessary - the idea is to vary the amount of compute used per forward pass by powers of 2). For the dataset size, multiply the fraction of the full dataset used by powers of 2 (i.e. 1, 1/2, 1/4, ...). To reduce noise, use a few random seeds and always use the full validation set.\n",
        "- The learning rate will need to vary with model size. Either tune it carefully for each model size, or use the rule of thumb that for Adam, the learning rate should be proportional to the initialization scale, i.e. `1/sqrt(fan_in)` for the standard Kaiming He initialization (which is what PyTorch generally uses by default).\n",
        "    - Note - `fan_in` refers to the variable $N_{in}$, which is `in_features` for a linear layer, and `in_channels * kernel_size * kernel_size` for a convolutional layer - in other words, the number of input parameters/activations we take a sumproduct over to get each output activation.\n",
        "- Plot the amount of compute used (on a log scale) against validation loss. The compute-efficient frontier should follow an approximate power law (straight line on a log scale).\n",
        "How does validation accuracy behave?\n",
        "- Study how the compute-efficient model size varies with compute. This should also follow an approximate power law. Try to estimate its exponent.\n",
        "- Repeat your entire experiment with 20% [dropout](https://pytorch.org/docs/stable/generated/torch.nn.Dropout.html) to see how this affects the scaling exponents.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CuxD5x8WKG7k"
      },
      "source": [
        "## Other WandB features: Saving & Logging\n",
        "\n",
        "Here are a few more Weights & Biases features you might also want to play around with:\n",
        "\n",
        "* [Logging media and objects in experiments](https://docs.wandb.ai/guides/track/log?fbclid=IwAR3NxKsGpEjZwq3vSwYkohZllMpBwxHgOCc_k0ByuD9XGUsi_Scf5ELvGsQ) - you'll be doing this during the RL week, and it's useful when you're training generative image models like VAEs and diffusion models.\n",
        "* [Code saving](https://docs.wandb.ai/guides/app/features/panels/code?fbclid=IwAR2BkaXbRf7cqEH8kc1VzqH_kOJWGxqjUb_JCBq_SCnXOx1oF-Rt-hHydb4) - this captures all python source code files in the current director and all subdirectories. It's great for reproducibility, and also for sharing your code with others.\n",
        "* [Saving and loading PyTorch models](https://wandb.ai/wandb/common-ml-errors/reports/How-to-Save-and-Load-Models-in-PyTorch--VmlldzozMjg0MTE?fbclid=IwAR1Y9MzFTxIiVBJG06b4ppitwKWR4H5_ncKyT2F_rR5Z_IHawmpBTKskPcQ) - you can do this easily using `torch.save`, but it's also possible to do this directly through Weights and Biases as an **artifact**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYHbcUwXxUFL"
      },
      "source": [
        "## Train your model from scratch\n",
        "\n",
        "Now that you understand how to run training loops, you can try a big one - training your ResNet from scratch on CIFAR-10 data!\n",
        "\n",
        "Here are some tips and suggestions for things you can experiment with:\n",
        "\n",
        "- First, try to reduce training time.\n",
        "    - Starting with a smaller ResNet than the full `ResNet34` is a good idea. Good hyperparameters on the small model tend to transfer over to the larger model because the architecture and the data are the same; the main difference is the larger model may require more regularization to prevent overfitting.\n",
        "    - Bad hyperparameters are usually clearly worse by the end of the first 1-2 epochs. If you can train for fewer epochs, you can test more hyperparameters with the same compute. You can manually abort runs that don't look promising, or you can try to do it automatically; [Hyperband](https://www.jmlr.org/papers/volume18/16-558/16-558.pdf) is a popular algorithm for this.\n",
        "    - Play with optimizations like [Automatic Mixed Precision](https://pytorch.org/docs/stable/amp.html) to see if you get a speed boost.\n",
        "- Random search for a decent learning rate and batch size combination that allows your model to mostly memorize (overfit) the training set.\n",
        "    - It's better to overfit at the start than underfit, because it means your model is capable of learning and has enough capacity.\n",
        "    - Learning rate is often the most important single hyperparameter, so it's important to get a good-enough value early.\n",
        "    - Eventually, you'll want a learning rate schedule. Usually, you'll start low and gradually increase, then gradually decrease but many other schedules are feasible. [Jeremy Jordan](https://www.jeremyjordan.me/nn-learning-rate/) has a good blog post on learning rates.\n",
        "    - Larger batch size increases GPU memory usage and doubling batch size [often allows doubling learning rate](https://arxiv.org/pdf/1706.02677.pdf), up to a point where this relationship breaks down. The heuristic is that larger batches give a more accurate estimate of the direction to update in. Note that on the test set, you can vary the batch size independently and usually the largest value that will fit on your GPU will be the most efficient.\n",
        "- Add regularization to reduce the amount of overfitting and train for longer to see if it's enough.\n",
        "    - Data augmention is the first thing to do - flipping the image horizontally and Cutout are known to be effective.\n",
        "    - Play with the label smoothing parameter to [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html).\n",
        "    - Try adding weight decay to Adam. This is a bit tricky - see this [fast.ai](https://www.fast.ai/2018/07/02/adam-weight-decay/) article if you want to do this, as well as the [PyTorch pseudocode](https://pytorch.org/docs/stable/generated/torch.optim.AdamW.html).\n",
        "- Try a bit of architecture search: play with various numbers of blocks and block groups. Or pick some fancy newfangled nonlinearity and see if it works better than ReLU.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ef-CmUlKxl0x"
      },
      "source": [
        "## The Optimizer's Curse\n",
        "\n",
        "The [optimizer's curse](https://www.lesswrong.com/posts/5gQLrJr2yhPzMCcni/the-optimizer-s-curse-and-how-to-beat-it) applies to tuning hyperparameters. The main take-aways are:\n",
        "\n",
        "- You can expect your best hyperparameter combination to actually underperform in the future. You chose it because it was the best on some metric, but that metric has an element of noise/luck, and the more combinations you test the larger this effect is.\n",
        "- Look at the overall trends and correlations in context and try to make sense of the values you're seeing. Just because you ran a long search process doesn't mean your best output is really the best.\n",
        "\n",
        "For more on this, see [Preventing \"Overfitting\" of Cross-Validation Data](https://ai.stanford.edu/~ang/papers/cv-final.pdf) by Andrew Ng.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxQDTJO9xo7N"
      },
      "source": [
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P4Srbvudw5Z3"
      },
      "source": [
        "`wandb` is an incredibly useful tool when training models, and you should find yourself using it a fair amount throughout this program. You can always return to this page of exercises if you forget how any part of it works.\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "f0502-wyRh8A",
        "Na3PD6GwlBSk",
        "LZqyzRF6yh8-",
        "k9dJxteKhMtz",
        "9wwTcPbIKG7j"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}